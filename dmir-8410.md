---
layout: default
---
**[Home](https://yvesmango.github.io/) >> [Projects](https://yvesmango.github.io/projects) >> [8410 Computational Text Analysis](https://yvesmango.github.io/dmir-8410) >> Jupyter Notebook**

# Subreddit Sentiment Analysis notebook

## Table of Contents

- [Introduction](#introduction)
- [Project Description](#project-description)
- [Data Description](#data-description)
- [Conclusion](#conclusion)
- [License](#license)


## Part I: Data Acquisition and Loading 
1. This is a two-part project in which involved data acquisition/loading in the first, and then analytics in the second.

## Part II: Analytics 

1. Write some test queries following the text vectors from Module 7.
1. Produce **interesting visualizations** of the linguistic data.
    * Try to look for trends (within a subreddit) and variations of topics across subreddits
    * Some comparative plots across feeds
1. Write a summary of your findings!

 
 

## Part II: Analytics 


```python
## Your code in this cell
## ------------------------
import re
import spacy
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline
```


```python
## Your code in this cell
## ------------------------

#Connecting to the database
%load_ext sql
%sql postgres://dsa_ro_user:readonly@pgsql.dsa.lan/dsa_student



```

```
 'Connected: dsa_ro_user@dsa_student'  
```




## Task 8: Produce interesting visualizations of the lingustic data.

 * Examples:
     * Try to look for trends (within a subreddit)
     * Topic variations across subreddits 
     * Some comparative plots across subreddits 



```python
%%sql

SELECT DISTINCT subreddit
FROM ydn3f.redditposts;
```

     * postgres://dsa_ro_user:***@pgsql.dsa.lan/dsa_student
    5 rows affected.





<table>
    <tr>
        <th>subreddit</th>
    </tr>
    <tr>
        <td>solotravel</td>
    </tr>
    <tr>
        <td>dataengineering</td>
    </tr>
    <tr>
        <td>LawSchool</td>
    </tr>
    <tr>
        <td>COVID19positive</td>
    </tr>
    <tr>
        <td>TwoSentenceHorror</td>
    </tr>
</table>




```python
## Your code in this cell
## ------------------------

credentials = "postgres://dsa_ro_user:readonly@pgsql.dsa.lan/dsa_student"


dataeng = pd.read_sql("""
            SELECT *
            FROM ydn3f.redditposts
            WHERE subreddit = 'dataengineering'
            """, con = credentials)

lawschool = pd.read_sql("""
            SELECT *
            FROM ydn3f.redditposts
            WHERE subreddit = 'LawSchool'
            """, con = credentials)

covid19 = pd.read_sql("""
            SELECT *
            FROM ydn3f.redditposts
            WHERE subreddit = 'COVID19positive'
            """, con = credentials)

twosentence = pd.read_sql("""
            SELECT *
            FROM ydn3f.redditposts
            WHERE subreddit = 'TwoSentenceHorror'
            """, con = credentials)

solotravel = pd.read_sql("""
            SELECT *
            FROM ydn3f.redditposts
            WHERE subreddit = 'solotravel'
            """, con = credentials)

```


```python
dataeng.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>151xsis</td>
      <td>567</td>
      <td>Data Scientists -- Ok, now I get it.</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>tarzanboy76</td>
      <td>dataengineering</td>
      <td>Discussion</td>
      <td>2023-07-17</td>
      <td>220</td>
      <td>a few days ago, our data scientist gave me som...</td>
      <td>0.035</td>
      <td>0.861</td>
      <td>0.104</td>
      <td>0.9340</td>
      <td>POS</td>
      <td>'access':193 'actual':62,110,147 'admin':192 '...</td>
      <td>'access':193 'actual':62,110,147 'admin':192 '...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10kl6lg</td>
      <td>374</td>
      <td>Finally got a job</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>1000gratitudepunches</td>
      <td>dataengineering</td>
      <td>Career</td>
      <td>2023-01-25</td>
      <td>100</td>
      <td>i did it! after 8 months of working as a budte...</td>
      <td>0.000</td>
      <td>0.950</td>
      <td>0.050</td>
      <td>0.5093</td>
      <td>POS</td>
      <td>'12':24 '400':20 '8':5 'applic':22 'believ':42...</td>
      <td>'12':24 '400':20 '8':5 'applic':22 'believ':42...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>yyh6l9</td>
      <td>381</td>
      <td>What are your favourite GitHub repos that show...</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>theoriginalmantooth</td>
      <td>dataengineering</td>
      <td>Discussion</td>
      <td>2022-11-18</td>
      <td>40</td>
      <td>looking to level up my skills and want to know...</td>
      <td>0.000</td>
      <td>0.899</td>
      <td>0.101</td>
      <td>0.5775</td>
      <td>POS</td>
      <td>'accounts/repos':20 'alreadi':46 'data':17 'di...</td>
      <td>'accounts/repos':20 'alreadi':46 'data':17 'di...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14663ur</td>
      <td>294</td>
      <td>r/dataengineering will be joining the blackout...</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>AutoModerator</td>
      <td>dataengineering</td>
      <td>Meta</td>
      <td>2023-06-10</td>
      <td>21</td>
      <td>[see here for the original r/dataengineering t...</td>
      <td>0.087</td>
      <td>0.840</td>
      <td>0.073</td>
      <td>-0.8688</td>
      <td>NEG</td>
      <td>'/)*.':536 '/hc/en-us/requests/new):':352 '/r/...</td>
      <td>'/)*.':536 '/hc/en-us/requests/new):':352 '/r/...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10fg07o</td>
      <td>286</td>
      <td>just got laid off (FAANG)</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>Foodwithfloyd</td>
      <td>dataengineering</td>
      <td>Career</td>
      <td>2023-01-18</td>
      <td>84</td>
      <td>hi all, its been a pretty awful day. two month...</td>
      <td>0.032</td>
      <td>0.808</td>
      <td>0.160</td>
      <td>0.9118</td>
      <td>POS</td>
      <td>'ago':11 'anoth':26 'anyon':98 'aw':7 'beyond'...</td>
      <td>'ago':11 'anoth':26 'anyon':98 'aw':7 'beyond'...</td>
    </tr>
  </tbody>
</table>
</div>




```python
lawschool.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>13c2x19</td>
      <td>6172</td>
      <td>I promised my mom on her death bed that I woul...</td>
      <td>https://www.reddit.com/r/LawSchool/comments/13...</td>
      <td>cinnamorolloing</td>
      <td>LawSchool</td>
      <td>None</td>
      <td>2023-05-08</td>
      <td>192</td>
      <td>this one is for you, mom.</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>0.0000</td>
      <td>NEU</td>
      <td>'mom':6 'one':2</td>
      <td>'mom':6 'one':2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14fhvdj</td>
      <td>1590</td>
      <td>Not in law school (Econ undergrad) but I am cu...</td>
      <td>https://www.reddit.com/r/LawSchool/comments/14...</td>
      <td>om-om</td>
      <td>LawSchool</td>
      <td>None</td>
      <td>2023-06-21</td>
      <td>74</td>
      <td></td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.000</td>
      <td>0.0000</td>
      <td>NEU</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>13dw7mo</td>
      <td>1531</td>
      <td>A Sigma Male Law School Schedule</td>
      <td>https://www.reddit.com/r/LawSchool/comments/13...</td>
      <td>Equivalent-Editor697</td>
      <td>LawSchool</td>
      <td>None</td>
      <td>2023-05-10</td>
      <td>110</td>
      <td>2:00 am- wake up2.05am-cold shower2.15am-break...</td>
      <td>0.026</td>
      <td>0.974</td>
      <td>0.000</td>
      <td>-0.2960</td>
      <td>NEG</td>
      <td>'-2':123 '00':2,124 '00am':42,64 '00am-arrive'...</td>
      <td>'-2':123 '00':2,124 '00am':42,64 '00am-arrive'...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>151geb6</td>
      <td>1458</td>
      <td>Sex during the bar?</td>
      <td>https://www.reddit.com/r/LawSchool/comments/15...</td>
      <td>Decent_Situation_952</td>
      <td>LawSchool</td>
      <td>None</td>
      <td>2023-07-16</td>
      <td>219</td>
      <td>iâ€™m sitting for the bar this month. during the...</td>
      <td>0.051</td>
      <td>0.890</td>
      <td>0.059</td>
      <td>-0.4836</td>
      <td>NEG</td>
      <td>'3l':22 'alreadi':76 'anoth':21 'bar':6 'bathr...</td>
      <td>'3l':22 'alreadi':76 'anoth':21 'bar':6 'bathr...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12k0vjz</td>
      <td>1282</td>
      <td>I passed the bar exam!</td>
      <td>https://www.reddit.com/r/LawSchool/comments/12...</td>
      <td>Organic-Ad-86</td>
      <td>LawSchool</td>
      <td>None</td>
      <td>2023-04-12</td>
      <td>74</td>
      <td>....and i'm stoked. that's all.</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.000</td>
      <td>0.0000</td>
      <td>NEU</td>
      <td>'m':3 'stoke':4</td>
      <td>'m':3 'stoke':4</td>
    </tr>
  </tbody>
</table>
</div>




```python
covid19.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>yjrg0a</td>
      <td>907</td>
      <td>Up vote if you're currently positive with your...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>Hailabigail</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Breakthrough</td>
      <td>2022-11-02</td>
      <td>311</td>
      <td>i'm seeing an overwhelming amount of posts wit...</td>
      <td>0.106</td>
      <td>0.727</td>
      <td>0.167</td>
      <td>0.7293</td>
      <td>POS</td>
      <td>'10':48 'amount':6,29 'breakthrough':31 'covid...</td>
      <td>'10':48 'amount':6,29 'breakthrough':31 'covid...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13p6qrm</td>
      <td>597</td>
      <td>Why is everyone pretending the pandemic disapp...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>marconas1_</td>
      <td>COVID19positive</td>
      <td>Rant</td>
      <td>2023-05-22</td>
      <td>271</td>
      <td>i work in a tech company, and it has become co...</td>
      <td>0.154</td>
      <td>0.776</td>
      <td>0.069</td>
      <td>-0.8343</td>
      <td>NEG</td>
      <td>'accept':66 'affect':33 'back':40 'becom':10 '...</td>
      <td>'accept':66 'affect':33 'back':40 'becom':10 '...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12lw075</td>
      <td>461</td>
      <td>What isâ€¦.happening here?</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>brutallyhonestkitten</td>
      <td>COVID19positive</td>
      <td>Rant</td>
      <td>2023-04-14</td>
      <td>201</td>
      <td>like the title says, i feel like i am living i...</td>
      <td>0.027</td>
      <td>0.853</td>
      <td>0.119</td>
      <td>0.9247</td>
      <td>POS</td>
      <td>'absolut':37 'alien':126 'altern':13 'anymor':...</td>
      <td>'absolut':37 'alien':126 'altern':13 'anymor':...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>zw72uc</td>
      <td>418</td>
      <td>This new variant was one of the worst experien...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>Throwawayacount5093</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Me</td>
      <td>2022-12-27</td>
      <td>145</td>
      <td>iâ€™m in my early twenties, fully vaxed and boos...</td>
      <td>0.121</td>
      <td>0.790</td>
      <td>0.089</td>
      <td>-0.8072</td>
      <td>NEG</td>
      <td>'104':120 '4':233 '60':206 '60mg':201 'abl':21...</td>
      <td>'104':120 '4':233 '60':206 '60mg':201 'abl':21...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>zji350</td>
      <td>396</td>
      <td>The pandemic's over they said. You don't need ...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>None</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Me</td>
      <td>2022-12-12</td>
      <td>216</td>
      <td>i haven't slept in nearly 40 hours, was in the...</td>
      <td>0.069</td>
      <td>0.839</td>
      <td>0.091</td>
      <td>0.2023</td>
      <td>POS</td>
      <td>'15':14 '40':7 '4x':77 'ach':54 'ash':34 'cong...</td>
      <td>'15':14 '40':7 '4x':77 'ach':54 'ash':34 'cong...</td>
    </tr>
  </tbody>
</table>
</div>




```python
display(twosentence.head())
twosentence = twosentence.drop(7)
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>15jp3hw</td>
      <td>10496</td>
      <td>Everyone on Earth was born with a dormant supe...</td>
      <td>https://www.reddit.com/r/TwoSentenceHorror/com...</td>
      <td>goshyarnit</td>
      <td>TwoSentenceHorror</td>
      <td>None</td>
      <td>2023-08-06</td>
      <td>371</td>
      <td>imagine my terror when my wife and son were mu...</td>
      <td>0.357</td>
      <td>0.643</td>
      <td>0.0</td>
      <td>-0.8750</td>
      <td>NEG</td>
      <td>'front':12 'imagin':1 'murder':10 'power':20 '...</td>
      <td>'front':12 'imagin':1 'murder':10 'power':20 '...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10lqxfg</td>
      <td>10265</td>
      <td>The captain has been banging on the walls for ...</td>
      <td>https://www.reddit.com/r/TwoSentenceHorror/com...</td>
      <td>Aco282</td>
      <td>TwoSentenceHorror</td>
      <td>None</td>
      <td>2023-01-26</td>
      <td>256</td>
      <td>i've counted several times, but all the space ...</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.0</td>
      <td>0.0000</td>
      <td>NEU</td>
      <td>'count':3 'insid':12 'sever':4 'space':9 'suit...</td>
      <td>'count':3 'insid':12 'sever':4 'space':9 'suit...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>y9wfa9</td>
      <td>10002</td>
      <td>She awoke feeling too warm, wet and itchy, so ...</td>
      <td>https://www.reddit.com/r/TwoSentenceHorror/com...</td>
      <td>normancrane</td>
      <td>TwoSentenceHorror</td>
      <td>None</td>
      <td>2022-10-21</td>
      <td>142</td>
      <td>then earth returned to sleep.</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.0</td>
      <td>0.0000</td>
      <td>NEU</td>
      <td>'earth':2 'return':3 'sleep':5</td>
      <td>'earth':2 'return':3 'sleep':5</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13oypfs</td>
      <td>9784</td>
      <td>I was beating the enemy in combat to a bloody ...</td>
      <td>https://www.reddit.com/r/TwoSentenceHorror/com...</td>
      <td>Gullible-Purpose-503</td>
      <td>TwoSentenceHorror</td>
      <td>None</td>
      <td>2023-05-22</td>
      <td>216</td>
      <td>but when i saw my daughters beaten face, and t...</td>
      <td>0.282</td>
      <td>0.718</td>
      <td>0.0</td>
      <td>-0.8338</td>
      <td>NEG</td>
      <td>'back':19 'beaten':7 'brought':18 'cri':15 'da...</td>
      <td>'back':19 'beaten':7 'brought':18 'cri':15 'da...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>yym3os</td>
      <td>9762</td>
      <td>â€œIf you take the red pill, you become a millio...</td>
      <td>https://www.reddit.com/r/TwoSentenceHorror/com...</td>
      <td>whiskerbiscuit2</td>
      <td>TwoSentenceHorror</td>
      <td>None</td>
      <td>2022-11-18</td>
      <td>360</td>
      <td>â€œif you take the blue pill youâ€¦.errrâ€¦.gosh itâ€™...</td>
      <td>0.000</td>
      <td>1.000</td>
      <td>0.0</td>
      <td>0.0000</td>
      <td>NEU</td>
      <td>'blue':5,18 'errr':8 'gosh':9 'long':14 'pick'...</td>
      <td>'blue':5,18 'errr':8 'gosh':9 'long':14 'pick'...</td>
    </tr>
  </tbody>
</table>
</div>



```python
solotravel.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16c1of1</td>
      <td>5356</td>
      <td>The number of old sex tourists in Bangkok is i...</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>Weekly-Patience4087</td>
      <td>solotravel</td>
      <td>None</td>
      <td>2023-09-07</td>
      <td>629</td>
      <td>i am currently in bangkok and the number of se...</td>
      <td>0.075</td>
      <td>0.857</td>
      <td>0.068</td>
      <td>0.2326</td>
      <td>POS</td>
      <td>'2':211 'advantag':143 'also':61 'area':20,185...</td>
      <td>'2':211 'advantag':143 'also':61 'area':20,185...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11zkusj</td>
      <td>2967</td>
      <td>I encountered my first begpacker today</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>Northerner6</td>
      <td>solotravel</td>
      <td>None</td>
      <td>2023-03-23</td>
      <td>332</td>
      <td>i encountered my first begpacker today. i was ...</td>
      <td>0.074</td>
      <td>0.876</td>
      <td>0.050</td>
      <td>-0.6554</td>
      <td>NEG</td>
      <td>'accent':38 'afford':176 'american':37 'approa...</td>
      <td>'accent':38 'afford':176 'american':37 'approa...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13uw9tn</td>
      <td>2205</td>
      <td>REMINDER: Unwanted sexual attention is NEVER O...</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>unsuspectingmuggle</td>
      <td>solotravel</td>
      <td>Accommodation</td>
      <td>2023-05-29</td>
      <td>301</td>
      <td>report people who make you feel unsafe!i've be...</td>
      <td>0.128</td>
      <td>0.826</td>
      <td>0.047</td>
      <td>-0.9464</td>
      <td>NEG</td>
      <td>'11':34 '25':254 '99.99':256 'alon':124,248 'a...</td>
      <td>'11':34 '25':254 '99.99':256 'alon':124,248 'a...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11ccux4</td>
      <td>2124</td>
      <td>I have been in India for a month and so far I ...</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>Big-Assist-5</td>
      <td>solotravel</td>
      <td>None</td>
      <td>2023-02-26</td>
      <td>476</td>
      <td>one of the times i was staying at a guest hous...</td>
      <td>0.165</td>
      <td>0.828</td>
      <td>0.007</td>
      <td>-0.9831</td>
      <td>NEG</td>
      <td>'30':20 'answer':63 'appar':29 'away':22 'basi...</td>
      <td>'30':20 'answer':63 'appar':29 'away':22 'basi...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>146y8eu</td>
      <td>1900</td>
      <td>The first time I have ever felt unsafe in SE A...</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>ihatemycohort</td>
      <td>solotravel</td>
      <td>Asia</td>
      <td>2023-06-11</td>
      <td>232</td>
      <td>i just had a complete scare. im still shaking ...</td>
      <td>0.102</td>
      <td>0.811</td>
      <td>0.087</td>
      <td>-0.9189</td>
      <td>NEG</td>
      <td>'1':554,1018 '10':71 '15':199 '2':64,562,986 '...</td>
      <td>'1':554,1018 '10':71 '15':199 '2':64,562,986 '...</td>
    </tr>
  </tbody>
</table>
</div>



### Sentiment counts of each subreddit


```python
dataeng['sentiment'].value_counts()
```




    POS    78
    NEG    17
    NEU     5
    Name: sentiment, dtype: int64




```python
lawschool['sentiment'].value_counts()
```




    POS    49
    NEG    38
    NEU    13
    Name: sentiment, dtype: int64




```python
covid19['sentiment'].value_counts()
```




    NEG    58
    POS    36
    NEU     6
    Name: sentiment, dtype: int64




```python
twosentence['sentiment'].value_counts()
```




    NEG    43
    POS    29
    NEU    27
    Name: sentiment, dtype: int64




```python
solotravel['sentiment'].value_counts()
```




    POS    71
    NEG    29
    Name: sentiment, dtype: int64



## Exploratory Visualizations

### Polarity distributions


```python
# Create a figure and axis
plt.figure(figsize=(10, 6))


# Plot the distribution of the 'compound' score for the 'dataeng' subreddit
sns.distplot(dataeng['compound'], color='green', kde=False)

# Set plot labels and title
plt.xlabel('Compound Score')
plt.ylabel('Frequency')
plt.title('Distribution of Compound Scores in r/dataengineering')

# Show the plot
plt.show()
```


![Imgur](https://i.imgur.com/NYsHYMd.png)



```python
# Create a figure and axis
plt.figure(figsize=(10, 6))

# Plot the distribution of the 'compound' score for the 'dataeng' subreddit
sns.distplot(lawschool['compound'], color='orange', kde=False)

# Set plot labels and title
plt.xlabel('Compound Score')
plt.ylabel('Frequency')
plt.title('Distribution of Compound Scores in r/lawschool')

# Show the plot
plt.show()
```


![Imgur](https://i.imgur.com/ufZ1bz1.png)



```python
# Create a figure and axis
plt.figure(figsize=(10, 6))

# Plot the distribution of the 'compound' score for the 'dataeng' subreddit
sns.distplot(covid19['compound'], color='brown', kde=False)

# Set plot labels and title
plt.xlabel('Compound Score')
plt.ylabel('Frequency')
plt.title('Distribution of Compound Scores in r/covid19')

# Show the plot
plt.show()
```


![Imgur](https://i.imgur.com/Dde0Evx.png)




```python
# Create a figure and axis
plt.figure(figsize=(10, 6))

# Plot the distribution of the 'compound' score for the 'dataeng' subreddit
sns.distplot(solotravel['compound'], color='purple', kde=False)

# Set plot labels and title
plt.xlabel('Compound Score')
plt.ylabel('Frequency')
plt.title('Distribution of Compound Scores in r/solotravel')

# Show the plot
plt.show()
```


![Imgur](https://i.imgur.com/q3abgtX.png)


### Lawschool subreddit


```python
sns.set(style='whitegrid')
```


```python
import nltk
from nltk import word_tokenize
from nltk import FreqDist
from nltk.corpus import stopwords

lawschool_list = []

for row in lawschool["content"]:
    lawschool_list.append(row)

lawschool_content = ' '.join(lawschool_list)

lawschool_tokens = word_tokenize(lawschool_content)
total_word_count = len(lawschool_tokens)

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Also need to remove some other random punctuations
other_removals = ["'", ";", "?", "(", ")", "'m", "'s", "n't", ":", "!", "`", '''"''', "'ve", "*", "`", ",", ""]
stop_words_updated = stop_words.union(other_removals)

# Filter out stopwords and short words
tokens_wo_stopwords = [word.lower() for word in lawschool_tokens if word.lower() not in stop_words_updated and len(word) > 2]
freq_dist = nltk.FreqDist(tokens_wo_stopwords)

# Calculate the percentage share of words
word_freq = freq_dist.most_common(10)
percentage_share = [(word, freq / total_word_count * 100) for word, freq in word_freq]

# Create the plot
plt.figure(figsize=(12, 6))
x, y = zip(*percentage_share)
plt.bar(x, y)
plt.xlabel("Words")
plt.ylabel("Percentage Share")
plt.xticks(size=15, rotation=75)
plt.show()

```

    [nltk_data] Downloading package stopwords to /home/ydn3f/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.



![Imgur](https://i.imgur.com/2BWI31a.png)



```python
from nltk.util import ngrams

def plot_ngram_percentage_share(tokens, num, total_word_count, num_results=25):
    ngram = list(ngrams(tokens, num))
    ngram_dist = nltk.FreqDist(ngram)
    
    # Calculate the percentage share of bigrams
    ngram_freq = ngram_dist.most_common(num_results)
    percentage_share = [(bigram, freq / total_word_count * 100) for bigram, freq in ngram_freq]

    # Create the plot
    x, y = zip(*percentage_share)
    plt.figure(figsize=(10, 6))
    plt.bar([" ".join(bigram) for bigram in x], y)
    plt.xlabel(f"Top {num_results} {num}-grams")
    plt.ylabel("Percentage Share")
    plt.xticks(fontsize=15, rotation=75)
    plt.show()

plot_ngram_percentage_share(tokens_wo_stopwords, 3, total_word_count, num_results=10)
```


![Imgur](https://i.imgur.com/qTkajaO.png)

The first chart doesn't really reveal anything beyond the obvious, so I decided to dig a little deeper by exploring the frequent tri-grams that occur in the dataset for the lawschool subreddit. I was immediately surprised to see the top 3 to be mirror reflections of one another. I'm not a law student so I don't really know why those 3 words are mentioned as a group often, but I'm curious to hear why. The rest of the tri-grams in the top 10 are pretty self-explanatory.
  

```python
#Topic Modelling

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation

count_vectorizer = CountVectorizer(stop_words='english')
term_frequency = count_vectorizer.fit_transform(lawschool_list)
feature_names = count_vectorizer.get_feature_names()

print(f"Shape of term freq matrix = {term_frequency.shape}")
print(f"Num of features identified = {len(feature_names)}")

#LDA model with 5 topics
lda = LatentDirichletAllocation(n_components=5, random_state=0)  
lda.fit(term_frequency)  

def display_topics(model, feature_names, no_top_words):
    for topic_idx, term_weights in enumerate(model.components_):
        
        sorted_indx = term_weights.argsort()

        topk_words = [feature_names[i] for i in sorted_indx[-no_top_words :]]
        print(f"Topic {topic_idx}:", end=None)
        print(";".join(topk_words))


display_topics(lda, feature_names, 10)
```

    Shape of term freq matrix = (100, 2340)
    Num of features identified = 2340
    Topic 0:
    need;ranking;student;people;students;just;law;tax;corporate;did
    Topic 1:
    doesn;westlaw;foster;partner;know;care;getting;school;bar;summer
    Topic 2:
    probably;school;say;did;people;law;like;just;student;gen
    Topic 3:
    students;firm;school;class;know;time;just;like;people;law
    Topic 4:
    got;like;people;firm;going;know;bar;just;school;law



```python
#TFIDF VEC

tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(lawschool_list)
tfidf_feature_names = tfidf_vectorizer.get_feature_names()

print(f"Shape of tfidf matrix = {tfidf.shape}")
print(f"Num of features identified = {len(tfidf_feature_names)}")

#6 topics
nmf = NMF(n_components=5, random_state=0)
nmf.fit(term_frequency)

#Top 10 words per topic
display_topics(nmf, tfidf_feature_names, 10)
```

    Shape of tfidf matrix = (100, 2340)
    Num of features identified = 2340
    Topic 0:
    feeling;don;measure;like;paperwork;adhd;just;tax;corporate;did
    Topic 1:
    hard;students;going;just;make;exam;prep;bar;school;law
    Topic 2:
    let;really;just;tax;law;real;firm;people;know;like
    Topic 3:
    probably;youre;law;okay;like;student;say;people;just;gen
    Topic 4:
    got;heard;event;offer;minecraft;firm;just;getting;summer;partner


    /opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).
      "'nndsvda' in 1.1 (renaming of 0.26)."), FutureWarning)

TFIDF Vectorizer seemed to detect more distinct topic selections in the lawschool subreddit. I can easily infer Topic 1 to be about exam preparation like the bar and all the difficult long hours studying for it. Topic 3 sounds like users are posting submissions related to being a student and maybe words of support or encouragement. Topic 4 can easily be described as discussion topics related to working at a firm over the summer or a job offer of sorts.

```python
# Lawschool Sentiment Over Time
lawschool['published'] = pd.to_datetime(lawschool['published'])

lawschool_monthly = lawschool.copy()
lawschool_monthly.set_index('published', inplace=True)
lawschool_monthly = lawschool_monthly.resample('W').agg({'neg': 'mean', 'neu': 'mean', 'pos': 'mean', 'compound': 'mean'})
```


```python
# Plot the sentiment scores over time
plt.figure(figsize=(16, 5))
sns.lineplot(x=lawschool_monthly.index, y=lawschool_monthly['neg'], label='Negative', linewidth=4)
sns.lineplot(x=lawschool_monthly.index, y=lawschool_monthly['neu'], label='Neutral', linewidth=4)
sns.lineplot(x=lawschool_monthly.index, y=lawschool_monthly['pos'], label='Positive', linewidth=4)
sns.lineplot(x=lawschool_monthly.index, y=lawschool_monthly['compound'], label='Compound', linewidth=3, color='gray', alpha=0.8, style=True, dashes=[(1,1)], legend=False)
plt.xlabel('Time')
plt.ylabel('Sentiment Score')
plt.title('Sentiment Analysis Over Time for r/lawschool (past 12 months)')
plt.legend()
plt.grid(True)
plt.show()
```


![Imgur](https://i.imgur.com/6HiUqAI.png)

For visualizing sentiment over time, I chose to use the line chart for interpretability. The most significant portion of the time-series is the month of march, especially at the beginning, where a substantial collection of negatively sentimented posts were submitted. Could it be the period before bar exams? ðŸ¤”
### Solotravel subreddit


```python
solotravel.head(3)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16c1of1</td>
      <td>5356</td>
      <td>The number of old sex tourists in Bangkok is i...</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>Weekly-Patience4087</td>
      <td>solotravel</td>
      <td>None</td>
      <td>2023-09-07</td>
      <td>629</td>
      <td>i am currently in bangkok and the number of se...</td>
      <td>0.075</td>
      <td>0.857</td>
      <td>0.068</td>
      <td>0.2326</td>
      <td>POS</td>
      <td>'2':211 'advantag':143 'also':61 'area':20,185...</td>
      <td>'2':211 'advantag':143 'also':61 'area':20,185...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11zkusj</td>
      <td>2967</td>
      <td>I encountered my first begpacker today</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>Northerner6</td>
      <td>solotravel</td>
      <td>None</td>
      <td>2023-03-23</td>
      <td>332</td>
      <td>i encountered my first begpacker today. i was ...</td>
      <td>0.074</td>
      <td>0.876</td>
      <td>0.050</td>
      <td>-0.6554</td>
      <td>NEG</td>
      <td>'accent':38 'afford':176 'american':37 'approa...</td>
      <td>'accent':38 'afford':176 'american':37 'approa...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>13uw9tn</td>
      <td>2205</td>
      <td>REMINDER: Unwanted sexual attention is NEVER O...</td>
      <td>https://www.reddit.com/r/solotravel/comments/1...</td>
      <td>unsuspectingmuggle</td>
      <td>solotravel</td>
      <td>Accommodation</td>
      <td>2023-05-29</td>
      <td>301</td>
      <td>report people who make you feel unsafe!i've be...</td>
      <td>0.128</td>
      <td>0.826</td>
      <td>0.047</td>
      <td>-0.9464</td>
      <td>NEG</td>
      <td>'11':34 '25':254 '99.99':256 'alon':124,248 'a...</td>
      <td>'11':34 '25':254 '99.99':256 'alon':124,248 'a...</td>
    </tr>
  </tbody>
</table>
</div>




```python
solotravel_list = []

for row in solotravel["content"]:
    solotravel_list.append(row)

solotravel_content = ' '.join(solotravel_list)

solotravel_tokens = word_tokenize(solotravel_content)
total_word_count = len(solotravel_tokens)

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Also need to remove some other random punctuations
other_removals = ["'", ";", "?", "(", ")", "'m", "'s", "n't", ":", "!", "`", '''"''', "'ve", "*", "`", ",", ""]
stop_words_updated = stop_words.union(other_removals)

# Filter out stopwords and short words
tokens_wo_stopwords = [word.lower() for word in solotravel_tokens if word.lower() not in stop_words_updated and len(word) > 2]
freq_dist = nltk.FreqDist(tokens_wo_stopwords)

# Calculate the percentage share of words
word_freq = freq_dist.most_common(10)
percentage_share = [(word, freq / total_word_count * 100) for word, freq in word_freq]

# Create the plot
plt.figure(figsize=(12, 6))
x, y = zip(*percentage_share)
plt.bar(x, y)
plt.xlabel("Words")
plt.ylabel("Percentage Share")
plt.xticks(size=15, rotation=75)
plt.show()
```

    [nltk_data] Downloading package stopwords to /home/ydn3f/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



![Imgur](https://i.imgur.com/vt1Q0b0.png)



```python
plot_ngram_percentage_share(tokens_wo_stopwords, 3, total_word_count, num_results=10)
```


![Imgur](https://i.imgur.com/FvnyKKW.png)


### {solotravel} Sentiment analysis by geographic location detected


```python
from collections import defaultdict

nlp = spacy.load("en_core_web_sm")
```


```python
# Function to extract location mentions
def extract_locations(text):
    doc = nlp(text)
    locations = [ent.text for ent in doc.ents if ent.label_ == "GPE"]
    return locations

```


```python
travel_posts = solotravel[["content", "neg", "neu", "pos", "compound"]]

# Define a dictionary to store sentiment scores by location
location_sentiment = defaultdict(list)

# Iterate through each post and extract location mentions
for index, row in travel_posts.iterrows():
    content = row["content"]
    sentiment = {
        "neg": row["neg"],
        "neu": row["neu"],
        "pos": row["pos"],
        "compound": row["compound"],
    }
    locations = extract_locations(content)
    for location in locations:
        location_sentiment[location].append(sentiment)

```


```python
# Calculate summary statistics for sentiment by location
location_summary = {}
for location, sentiment_scores in location_sentiment.items():
    num_posts = len(sentiment_scores)
    if num_posts > 0:
        summary = {
            "num_posts": num_posts,
            "avg_neg": sum(score["neg"] for score in sentiment_scores) / num_posts,
            "avg_neu": sum(score["neu"] for score in sentiment_scores) / num_posts,
            "avg_pos": sum(score["pos"] for score in sentiment_scores) / num_posts,
            "avg_compound": sum(score["compound"] for score in sentiment_scores) / num_posts,
        }
        location_summary[location] = summary
```


```python
geo_sentiments = pd.DataFrame.from_dict(location_summary)
```


```python
geo_sentiments = geo_sentiments.transpose().rename_axis('geo-entity').reset_index()
```


```python
# Sort the DataFrame and select the top and bottom rows
sorted_geo_sentiments = geo_sentiments.query("num_posts > 5").sort_values(by='avg_compound') # minimum of 5 self-post
top_geo_sentiments = sorted_geo_sentiments.head(5)
bottom_geo_sentiments = sorted_geo_sentiments.tail(5)
```


```python
tb_geo_sentiments = pd.concat([top_geo_sentiments, bottom_geo_sentiments])

colors = ['#2E8BC0' if avg_compound > 0 else '#AE0000' for avg_compound in tb_geo_sentiments['avg_compound']]

# Create the bar chart
plt.figure(figsize=(12, 6))  # Adjust the figure size as needed
sns.barplot(y='avg_compound', x='geo-entity', data=tb_geo_sentiments, palette=colors)

# Add the abline for y=0 (neutral sentiment)
plt.axhline(0, color='black', linewidth=2, linestyle='-')

# Customize the labels and titles
plt.xlabel('Average Compound Sentiment Score')
plt.ylabel('Location')
plt.title('Average Compound Sentiment by Geo-entity')

# Display the chart
plt.show()
```


![Imgur](https://i.imgur.com/gQql5gk.png)


### Deeper look at the disparity of scores among the east vs west divide


```python
# Create an empty list to store the records
location_data = []

# Iterate through location_sentiment and convert it into records
for location, sentiment_scores in location_sentiment.items():
    for sentiment_score in sentiment_scores:
        record = {
            'location': location,
            'neg': sentiment_score['neg'],
            'neu': sentiment_score['neu'],
            'pos': sentiment_score['pos'],
            'compound': sentiment_score['compound']
        }
        location_data.append(record)

# Create a DataFrame from the list of records
location_df = pd.DataFrame(location_data)
```


```python
# List of specified countries
specified_countries = ['india', 'japan', 'thailand', 'vietnam', 'paris', 'romania', 'berlin', 'venice', 'madrid', 'rome']

# Filter the DataFrame to include only the specified countries
popular_countries = location_df[location_df['location'].isin(specified_countries)]
```


```python
# List of sentiment columns
sentiment_columns = ['compound', 'neg', 'neu', 'pos']

# Create box plots
plt.figure(figsize=(12, 8))
for sentiment_column in sentiment_columns:
    sns.boxplot(data=popular_countries, x='location', y=sentiment_column, palette='Set3')
    plt.xlabel('Countries')
    plt.ylabel(sentiment_column.capitalize() + ' Score')
    plt.title('Sentiment Analysis by Geo-entity')
    plt.xticks(rotation=45)
    plt.show()
```


![Imgur](https://i.imgur.com/NEe2LYn.png)



![Imgur](https://i.imgur.com/qFuq0hL.png)



![Imgur](https://i.imgur.com/qh8qBgL.png)



![Imgur](https://i.imgur.com/1HGshNI.png)


### Covid19 subreddit


```python
covid19.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>yjrg0a</td>
      <td>907</td>
      <td>Up vote if you're currently positive with your...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>Hailabigail</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Breakthrough</td>
      <td>2022-11-02</td>
      <td>311</td>
      <td>i'm seeing an overwhelming amount of posts wit...</td>
      <td>0.106</td>
      <td>0.727</td>
      <td>0.167</td>
      <td>0.7293</td>
      <td>POS</td>
      <td>'10':48 'amount':6,29 'breakthrough':31 'covid...</td>
      <td>'10':48 'amount':6,29 'breakthrough':31 'covid...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13p6qrm</td>
      <td>597</td>
      <td>Why is everyone pretending the pandemic disapp...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>marconas1_</td>
      <td>COVID19positive</td>
      <td>Rant</td>
      <td>2023-05-22</td>
      <td>271</td>
      <td>i work in a tech company, and it has become co...</td>
      <td>0.154</td>
      <td>0.776</td>
      <td>0.069</td>
      <td>-0.8343</td>
      <td>NEG</td>
      <td>'accept':66 'affect':33 'back':40 'becom':10 '...</td>
      <td>'accept':66 'affect':33 'back':40 'becom':10 '...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12lw075</td>
      <td>461</td>
      <td>What isâ€¦.happening here?</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>brutallyhonestkitten</td>
      <td>COVID19positive</td>
      <td>Rant</td>
      <td>2023-04-14</td>
      <td>201</td>
      <td>like the title says, i feel like i am living i...</td>
      <td>0.027</td>
      <td>0.853</td>
      <td>0.119</td>
      <td>0.9247</td>
      <td>POS</td>
      <td>'absolut':37 'alien':126 'altern':13 'anymor':...</td>
      <td>'absolut':37 'alien':126 'altern':13 'anymor':...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>zw72uc</td>
      <td>418</td>
      <td>This new variant was one of the worst experien...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>Throwawayacount5093</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Me</td>
      <td>2022-12-27</td>
      <td>145</td>
      <td>iâ€™m in my early twenties, fully vaxed and boos...</td>
      <td>0.121</td>
      <td>0.790</td>
      <td>0.089</td>
      <td>-0.8072</td>
      <td>NEG</td>
      <td>'104':120 '4':233 '60':206 '60mg':201 'abl':21...</td>
      <td>'104':120 '4':233 '60':206 '60mg':201 'abl':21...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>zji350</td>
      <td>396</td>
      <td>The pandemic's over they said. You don't need ...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>None</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Me</td>
      <td>2022-12-12</td>
      <td>216</td>
      <td>i haven't slept in nearly 40 hours, was in the...</td>
      <td>0.069</td>
      <td>0.839</td>
      <td>0.091</td>
      <td>0.2023</td>
      <td>POS</td>
      <td>'15':14 '40':7 '4x':77 'ach':54 'ash':34 'cong...</td>
      <td>'15':14 '40':7 '4x':77 'ach':54 'ash':34 'cong...</td>
    </tr>
  </tbody>
</table>
</div>




```python
covid19_list = []

for row in covid19["content"]:
    covid19_list.append(row)

covid19_content = ' '.join(covid19_list)

covid19_tokens = word_tokenize(covid19_content)
total_word_count = len(covid19_tokens)

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Also need to remove some other random punctuations
other_removals = ["'", ";", "?", "(", ")", "'m", "'s", "n't", ":", "!", "`", '''"''', "'ve", "*", "`", ",", ""]
stop_words_updated = stop_words.union(other_removals)

# Filter out stopwords and short words
tokens_wo_stopwords = [word.lower() for word in covid19_tokens if word.lower() not in stop_words_updated and len(word) > 2]
freq_dist = nltk.FreqDist(tokens_wo_stopwords)

# Calculate the percentage share of words
word_freq = freq_dist.most_common(10)
percentage_share = [(word, freq / total_word_count * 100) for word, freq in word_freq]

# Create the plot
plt.figure(figsize=(12, 6))
x, y = zip(*percentage_share)
plt.bar(x, y)
plt.xlabel("Words")
plt.ylabel("Percentage Share")
plt.xticks(size=15, rotation=75)
plt.show()
```

    [nltk_data] Downloading package stopwords to /home/ydn3f/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



![Imgur](https://i.imgur.com/7pgtXYL.png)



```python
plot_ngram_percentage_share(tokens_wo_stopwords, 3, total_word_count, num_results=10)
```


![Imgur](https://i.imgur.com/Sgpc8HU.png)



```python
covid19.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>yjrg0a</td>
      <td>907</td>
      <td>Up vote if you're currently positive with your...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>Hailabigail</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Breakthrough</td>
      <td>2022-11-02</td>
      <td>311</td>
      <td>i'm seeing an overwhelming amount of posts wit...</td>
      <td>0.106</td>
      <td>0.727</td>
      <td>0.167</td>
      <td>0.7293</td>
      <td>POS</td>
      <td>'10':48 'amount':6,29 'breakthrough':31 'covid...</td>
      <td>'10':48 'amount':6,29 'breakthrough':31 'covid...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>13p6qrm</td>
      <td>597</td>
      <td>Why is everyone pretending the pandemic disapp...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>marconas1_</td>
      <td>COVID19positive</td>
      <td>Rant</td>
      <td>2023-05-22</td>
      <td>271</td>
      <td>i work in a tech company, and it has become co...</td>
      <td>0.154</td>
      <td>0.776</td>
      <td>0.069</td>
      <td>-0.8343</td>
      <td>NEG</td>
      <td>'accept':66 'affect':33 'back':40 'becom':10 '...</td>
      <td>'accept':66 'affect':33 'back':40 'becom':10 '...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>12lw075</td>
      <td>461</td>
      <td>What isâ€¦.happening here?</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>brutallyhonestkitten</td>
      <td>COVID19positive</td>
      <td>Rant</td>
      <td>2023-04-14</td>
      <td>201</td>
      <td>like the title says, i feel like i am living i...</td>
      <td>0.027</td>
      <td>0.853</td>
      <td>0.119</td>
      <td>0.9247</td>
      <td>POS</td>
      <td>'absolut':37 'alien':126 'altern':13 'anymor':...</td>
      <td>'absolut':37 'alien':126 'altern':13 'anymor':...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>zw72uc</td>
      <td>418</td>
      <td>This new variant was one of the worst experien...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>Throwawayacount5093</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Me</td>
      <td>2022-12-27</td>
      <td>145</td>
      <td>iâ€™m in my early twenties, fully vaxed and boos...</td>
      <td>0.121</td>
      <td>0.790</td>
      <td>0.089</td>
      <td>-0.8072</td>
      <td>NEG</td>
      <td>'104':120 '4':233 '60':206 '60mg':201 'abl':21...</td>
      <td>'104':120 '4':233 '60':206 '60mg':201 'abl':21...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>zji350</td>
      <td>396</td>
      <td>The pandemic's over they said. You don't need ...</td>
      <td>https://www.reddit.com/r/COVID19positive/comme...</td>
      <td>None</td>
      <td>COVID19positive</td>
      <td>Tested Positive - Me</td>
      <td>2022-12-12</td>
      <td>216</td>
      <td>i haven't slept in nearly 40 hours, was in the...</td>
      <td>0.069</td>
      <td>0.839</td>
      <td>0.091</td>
      <td>0.2023</td>
      <td>POS</td>
      <td>'15':14 '40':7 '4x':77 'ach':54 'ash':34 'cong...</td>
      <td>'15':14 '40':7 '4x':77 'ach':54 'ash':34 'cong...</td>
    </tr>
  </tbody>
</table>
</div>




```python
covid19['published'] = pd.to_datetime(covid19['published'])
```


```python
from nltk import bigrams, trigrams

#list(bigrams(tokens_wo_stopwords))
```


```python
nltk.FreqDist(list(bigrams(tokens_wo_stopwords)))
```




    FreqDist({('feel', 'like'): 27, ('tested', 'positive'): 20, ('felt', 'like'): 16, ('first', 'time'): 14, ('got', 'covid'): 14, ('taste', 'smell'): 13, ('feels', 'like'): 12, ('wearing', 'mask'): 11, ('sense', 'smell'): 11, ('last', 'week'): 10, ...})




```python
# Convert the 'published' column to datetime
covid19['published'] = pd.to_datetime(covid19['published'])

# Define a time period for grouping, e.g., 'W' for weekly
covid19['time_period'] = covid19['published'].dt.isocalendar().week

# Initialize a list to store tokenized text for each time period
tokenized_by_time = []

# Iterate over time periods and tokenize the text for each period
for time_period, group in covid19.groupby('time_period'):
    covid19_content = ' '.join(group['content'])
    covid19_tokens = word_tokenize(covid19_content)
    tokens_wo_stopwords = [word.lower() for word in covid19_tokens if word.lower() not in stop_words_updated and len(word) > 2]
    tokenized_by_time.append(tokens_wo_stopwords)
```


```python
top_bigrams_by_time = []

# Iterate over time periods and calculate the top 3 bigrams for each period
for tokens in tokenized_by_time:
    bigram_fd = FreqDist(ngrams(tokens, 2))
    top_bigrams = bigram_fd.most_common(3)
    top_bigrams_by_time.append(top_bigrams)
```


```python
df = pd.DataFrame({'time_period': covid19['time_period'].unique(), 'Top Bigrams': top_bigrams_by_time})
```


```python
top_bigrams_by_time[0]
```




    [(('give', 'space'), 2), (('work', 'get'), 2), (('time', 'covid'), 2)]




```python
# Initialize lists to store data for the DataFrame
time_periods = []
bigrams = []
counters = []

# Iterate over time periods and bigrams to extract data for the DataFrame
for i, time_period in enumerate(covid19['time_period'].unique()):
    for bigram, counter in top_bigrams_by_time[i]:
        time_periods.append(time_period)
        bigrams.append(' '.join(bigram))
        counters.append(counter)

# Create a DataFrame with the extracted data
df = pd.DataFrame({'time_period': time_periods, 'Bigram': bigrams, 'Counter': counters})
```


```python
df.sort_values('Counter', ascending=False).head(10)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time_period</th>
      <th>Bigram</th>
      <th>Counter</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>96</th>
      <td>7</td>
      <td>dry cough</td>
      <td>7</td>
    </tr>
    <tr>
      <th>97</th>
      <td>7</td>
      <td>taste smell</td>
      <td>6</td>
    </tr>
    <tr>
      <th>98</th>
      <td>7</td>
      <td>night sweats</td>
      <td>5</td>
    </tr>
    <tr>
      <th>81</th>
      <td>38</td>
      <td>high fever</td>
      <td>5</td>
    </tr>
    <tr>
      <th>82</th>
      <td>38</td>
      <td>pretty much</td>
      <td>4</td>
    </tr>
    <tr>
      <th>25</th>
      <td>42</td>
      <td>wearing mask</td>
      <td>4</td>
    </tr>
    <tr>
      <th>24</th>
      <td>42</td>
      <td>still one</td>
      <td>4</td>
    </tr>
    <tr>
      <th>105</th>
      <td>24</td>
      <td>feels like</td>
      <td>4</td>
    </tr>
    <tr>
      <th>60</th>
      <td>27</td>
      <td>nasal spray</td>
      <td>4</td>
    </tr>
    <tr>
      <th>63</th>
      <td>14</td>
      <td>feel like</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div>




```python
display(df.shape)

# Filter the DataFrame to include only bigrams containing 'covid'
covid_bigrams = df[df['Bigram'].str.contains('covid|dry cough|taste smell|night sweats|nasal spray|high fever')]

    
# Define a color mapping for each bigram
def map_color(bigram):
    if 'covid' in bigram:
        return 'teal'
    elif 'dry cough' in bigram:
        return 'red'
    elif 'taste smell' in bigram:
        return 'green'
    elif 'night sweats' in bigram:
        return 'orange'
    elif 'nasal spray' in bigram:
        return 'hotpink'
    elif 'high fever' in bigram:
        return 'maroon'
    else:
        return 'gray'  # Default color for unmatched bigrams
```


    (114, 3)



```python
covid_bigrams = covid_bigrams.assign(Color=covid_bigrams['Bigram'].apply(map_color))
```


```python
import matplotlib.patches as mpatches

# Create the scatter plot with jitter and alpha for the filtered data
plt.figure(figsize=(14, 6))

for bigram in covid_bigrams['Bigram'].unique():
    subset = covid_bigrams[covid_bigrams['Bigram'] == bigram]
    color = map_color(bigram)
    jitter = np.random.normal(0, 0.9, len(subset))  # Add jitter for each unique bigram
    plt.scatter(
        subset['time_period'] + jitter,  # Match the length of jitter to the subset
        subset['Counter'],
        s=400,  # Adjust the size here (e.g., 100)
        c=color,  # Set the color based on the mapping
        alpha=0.6,
        edgecolor='black',  # Add a black outline
        linewidth=1.5,  # Control the thickness of the outline
        label=bigram
    )

plt.xlabel('Week #')
plt.ylabel('Bigram Counter')
plt.title('Top Bigrams Over Time (Week 0 = Oct 2022)')
plt.xticks()
plt.grid(True)

# Create custom legend patches for each color category
legend_labels = [
    mpatches.Patch(color='teal', label='Covid-Related Bigrams'),
    mpatches.Patch(color='red', label='Dry Cough Bigrams'),
    mpatches.Patch(color='green', label='Taste and Smell Bigrams'),
    mpatches.Patch(color='orange', label='Night Sweats Bigrams'),
    mpatches.Patch(color='hotpink', label='Nasal Spray Bigrams'),
    mpatches.Patch(color='maroon', label='High Fever Bigrams')
]

plt.legend(handles=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()
```


![Imgur](https://i.imgur.com/B713X93.png)


### Data engineering subreddit


```python
dataeng.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>score</th>
      <th>title</th>
      <th>link</th>
      <th>author</th>
      <th>subreddit</th>
      <th>flair</th>
      <th>published</th>
      <th>comments</th>
      <th>content</th>
      <th>neg</th>
      <th>neu</th>
      <th>pos</th>
      <th>compound</th>
      <th>sentiment</th>
      <th>content_tsv_gin</th>
      <th>content_tsv_gist</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>151xsis</td>
      <td>567</td>
      <td>Data Scientists -- Ok, now I get it.</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>tarzanboy76</td>
      <td>dataengineering</td>
      <td>Discussion</td>
      <td>2023-07-17</td>
      <td>220</td>
      <td>a few days ago, our data scientist gave me som...</td>
      <td>0.035</td>
      <td>0.861</td>
      <td>0.104</td>
      <td>0.9340</td>
      <td>POS</td>
      <td>'access':193 'actual':62,110,147 'admin':192 '...</td>
      <td>'access':193 'actual':62,110,147 'admin':192 '...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10kl6lg</td>
      <td>374</td>
      <td>Finally got a job</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>1000gratitudepunches</td>
      <td>dataengineering</td>
      <td>Career</td>
      <td>2023-01-25</td>
      <td>100</td>
      <td>i did it! after 8 months of working as a budte...</td>
      <td>0.000</td>
      <td>0.950</td>
      <td>0.050</td>
      <td>0.5093</td>
      <td>POS</td>
      <td>'12':24 '400':20 '8':5 'applic':22 'believ':42...</td>
      <td>'12':24 '400':20 '8':5 'applic':22 'believ':42...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>yyh6l9</td>
      <td>381</td>
      <td>What are your favourite GitHub repos that show...</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>theoriginalmantooth</td>
      <td>dataengineering</td>
      <td>Discussion</td>
      <td>2022-11-18</td>
      <td>40</td>
      <td>looking to level up my skills and want to know...</td>
      <td>0.000</td>
      <td>0.899</td>
      <td>0.101</td>
      <td>0.5775</td>
      <td>POS</td>
      <td>'accounts/repos':20 'alreadi':46 'data':17 'di...</td>
      <td>'accounts/repos':20 'alreadi':46 'data':17 'di...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14663ur</td>
      <td>294</td>
      <td>r/dataengineering will be joining the blackout...</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>AutoModerator</td>
      <td>dataengineering</td>
      <td>Meta</td>
      <td>2023-06-10</td>
      <td>21</td>
      <td>[see here for the original r/dataengineering t...</td>
      <td>0.087</td>
      <td>0.840</td>
      <td>0.073</td>
      <td>-0.8688</td>
      <td>NEG</td>
      <td>'/)*.':536 '/hc/en-us/requests/new):':352 '/r/...</td>
      <td>'/)*.':536 '/hc/en-us/requests/new):':352 '/r/...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10fg07o</td>
      <td>286</td>
      <td>just got laid off (FAANG)</td>
      <td>https://www.reddit.com/r/dataengineering/comme...</td>
      <td>Foodwithfloyd</td>
      <td>dataengineering</td>
      <td>Career</td>
      <td>2023-01-18</td>
      <td>84</td>
      <td>hi all, its been a pretty awful day. two month...</td>
      <td>0.032</td>
      <td>0.808</td>
      <td>0.160</td>
      <td>0.9118</td>
      <td>POS</td>
      <td>'ago':11 'anoth':26 'anyon':98 'aw':7 'beyond'...</td>
      <td>'ago':11 'anoth':26 'anyon':98 'aw':7 'beyond'...</td>
    </tr>
  </tbody>
</table>
</div>




```python
#Topic modelling

dataeng_list = []

for row in dataeng["content"]:
    dataeng_list.append(row)

dataeng_content = ' '.join(dataeng_list)

dataeng_tokens = word_tokenize(dataeng_content)
total_word_count = len(dataeng_tokens)

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Also need to remove some other random punctuations
other_removals = ["'", ";", "?", "(", ")", "'m", "'s", "n't", ":", "!", "`", '''"''', "'ve", "*", "`", ",", ""]
stop_words_updated = stop_words.union(other_removals)

# Filter out stopwords and short words
tokens_wo_stopwords = [word.lower() for word in dataeng_tokens if word.lower() not in stop_words_updated and len(word) > 2]
freq_dist = nltk.FreqDist(tokens_wo_stopwords)

# Calculate the percentage share of words
word_freq = freq_dist.most_common(10)
percentage_share = [(word, freq / total_word_count * 100) for word, freq in word_freq]

# Create the plot
plt.figure(figsize=(12, 6))
x, y = zip(*percentage_share)
plt.bar(x, y)
plt.xlabel("Words")
plt.ylabel("Percentage Share")
plt.xticks(size=15, rotation=75)
plt.show()
```

    [nltk_data] Downloading package stopwords to /home/ydn3f/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



![Imgur](https://i.imgur.com/Wjk2epE.png)



```python
plot_ngram_percentage_share(tokens_wo_stopwords, 3, total_word_count, num_results=10)
```


![Imgur](https://i.imgur.com/7uJkqYd.png)



```python
#Topic Modelling

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation

count_vectorizer = CountVectorizer(stop_words='english')
term_frequency = count_vectorizer.fit_transform(dataeng_list)
feature_names = count_vectorizer.get_feature_names()

print(f"Shape of term freq matrix = {term_frequency.shape}")
print(f"Num of features identified = {len(feature_names)}")

#LDA model with 5 topics
lda = LatentDirichletAllocation(n_components=5, random_state=0)  
lda.fit(term_frequency)  

def display_topics(model, feature_names, no_top_words):
    for topic_idx, term_weights in enumerate(model.components_):
        
        sorted_indx = term_weights.argsort()

        topk_words = [feature_names[i] for i in sorted_indx[-no_top_words :]]
        print(f"Topic {topic_idx}:", end=None)
        print(";".join(topk_words))


display_topics(lda, feature_names, 10)
```

    Shape of term freq matrix = (100, 2888)
    Num of features identified = 2888
    Topic 0:
    pipeline;sql;like;use;just;api;need;https;cloud;data
    Topic 1:
    isn;app;databricks;data;make;comments;www;https;com;reddit
    Topic 2:
    years;engineering;people;job;just;time;sql;like;company;data
    Topic 3:
    years;team;ve;learn;know;really;just;like;databricks;data
    Topic 4:
    team;blog;dbt;snowflake;databricks;spark;data;instacart;com;https



```python
#TFIDF VEC

tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf = tfidf_vectorizer.fit_transform(dataeng_list)
tfidf_feature_names = tfidf_vectorizer.get_feature_names()

print(f"Shape of tfidf matrix = {tfidf.shape}")
print(f"Num of features identified = {len(tfidf_feature_names)}")

#6 topics
nmf = NMF(n_components=5, random_state=0)
nmf.fit(term_frequency)

#Top 10 words per topic
display_topics(nmf, tfidf_feature_names, 10)
```

    Shape of tfidf matrix = (100, 2888)
    Num of features identified = 2888


    /opt/conda/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).
      "'nndsvda' in 1.1 (renaming of 0.26)."), FutureWarning)


    Topic 0:
    support;isn;official;app;make;comments;www;https;com;reddit
    Topic 1:
    business;work;files;engineering;years;ve;sql;learn;just;data
    Topic 2:
    time;data;excel;just;extremely;team;people;job;like;company
    Topic 3:
    spark;etl;understand;really;platform;cloud;lot;data;snowflake;databricks
    Topic 4:
    blog;data;course;spark;snowflake;www;instacart;databricks;com;https



```python
# Sample function to assign topics based on keywords
def assign_topic(content):
    if "career" in content.lower():
        return "Career"
    elif "projects" in content.lower():
        return "Projects"
    elif "personal" in content.lower():
        return "Personal"
    elif "people" in content.lower():
        return "People"
    elif "company" in content.lower():
        return "Company"  
    else:
        return "Other"

def assign_topic_data(content):
    if "sql" in content.lower():
        return "SQL"
    elif "snowflake" in content.lower():
        return "Snowflake"
    elif "databricks" in content.lower():
        return "Databricks"
    elif "apache" in content.lower():
        return "Apache"
    elif "Spark" in content.lower():
        return "Spark"    
    else:
        return "Other"
```


```python
topics = ["Career", "Projects", "Personal"]
data_topics = ['sql', 'databricks', 'snowflake', 'people', 'company']
```


```python
    # Apply the function to the DataFrame
dataeng['topic'] = dataeng['content'].apply(assign_topic)
dataeng['data_topic'] = dataeng['content'].apply(assign_topic_data)
```


```python
# Group by topic and calculate the average sentiment score
role_sentiments = dataeng.groupby('topic')['compound'].mean().reset_index()
```


```python
# Group by topic and calculate summary statistics
topic_summary = dataeng.groupby('topic').agg({
    'compound': ['mean', 'min', 'max', 'median', 'std'],
    'neg': 'mean',
    'neu': 'mean',
    'pos': 'mean'
}).reset_index()

# Flatten the multi-index columns
topic_summary.columns = ['_'.join(col).strip() for col in topic_summary.columns.values]

```


```python
# Define the sentiment score columns
sentiment_columns = ['compound', 'neg', 'neu', 'pos']

# Create box plots
plt.figure(figsize=(12, 8))
for sentiment_column in sentiment_columns:
    sns.boxplot(data=dataeng, x='topic', y=sentiment_column, palette='Set2')
    plt.xlabel('Topics')
    plt.ylabel(sentiment_column.capitalize() + ' Score')
    plt.title('Sentiment Analysis by Topic')
    plt.xticks(rotation=45)
    plt.show()

```


![Imgur](https://i.imgur.com/NJUZBNl.png)



![Imgur](https://i.imgur.com/14WrfWX.png)



![Imgur](https://i.imgur.com/bGdJg9p.png)



![Imgur](https://i.imgur.com/6cPygZR.png)



```python
tools_sentiments = dataeng.groupby('data_topic')['compound'].mean().reset_index()
```


```python
# Define the sentiment score columns
sentiment_columns = ['compound', 'neg', 'neu', 'pos']

# Create box plots
plt.figure(figsize=(12, 8))
for sentiment_column in sentiment_columns:
    sns.boxplot(data=dataeng, x='data_topic', y=sentiment_column, palette='Set3')
    plt.xlabel('Topics')
    plt.ylabel(sentiment_column.capitalize() + ' Score')
    plt.title('Sentiment Analysis by Topic')
    plt.xticks(rotation=45)
    plt.show()

```


![Imgur](https://i.imgur.com/0NDqZjE.png)



![Imgur](https://i.imgur.com/pzwmTUV.png)



![Imgur](https://i.imgur.com/qKn37ws.png)



![Imgur](https://i.imgur.com/LOvXJNg.png)


### Two sentence horror


```python
tsh_list = []

for row in twosentence["content"]:
    tsh_list.append(row)

tsh_content = ' '.join(tsh_list)

tsh_tokens = word_tokenize(tsh_content)
total_word_count = len(tsh_tokens)

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Also need to remove some other random punctuations
other_removals = ["'", ";", "?", "(", ")", "'m", "'s", "n't", ":", "!", "`", '''"''', "'ve", "*", "`", ",", ""]
stop_words_updated = stop_words.union(other_removals)

# Filter out stopwords and short words
tokens_wo_stopwords = [word.lower() for word in tsh_tokens if word.lower() not in stop_words_updated and len(word) > 2]
freq_dist = nltk.FreqDist(tokens_wo_stopwords)

# Calculate the percentage share of words
word_freq = freq_dist.most_common(10)
percentage_share = [(word, freq / total_word_count * 100) for word, freq in word_freq]

# Create the plot
plt.figure(figsize=(12, 6))
x, y = zip(*percentage_share)
plt.bar(x, y)
plt.xlabel("Words")
plt.ylabel("Percentage Share")
plt.xticks(size=15, rotation=75)
plt.show()
```

    [nltk_data] Downloading package stopwords to /home/ydn3f/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!



![png](output_91_1.png)



```python
plot_ngram_percentage_share(tokens_wo_stopwords, 3, total_word_count, num_results=10)
```


![png](output_92_0.png)



```python
#Topic Modelling

from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation

count_vectorizer = CountVectorizer(stop_words='english')
term_frequency = count_vectorizer.fit_transform(tsh_list)
feature_names = count_vectorizer.get_feature_names()

print(f"Shape of term freq matrix = {term_frequency.shape}")
print(f"Num of features identified = {len(feature_names)}")

#LDA model with 5 topics
lda = LatentDirichletAllocation(n_components=5, random_state=0)  
lda.fit(term_frequency)  

def display_topics(model, feature_names, no_top_words):
    for topic_idx, term_weights in enumerate(model.components_):
        
        sorted_indx = term_weights.argsort()

        topk_words = [feature_names[i] for i in sorted_indx[-no_top_words :]]
        print(f"Topic {topic_idx}:", end=None)
        print(";".join(topk_words))


display_topics(lda, feature_names, 10)
```

    Shape of term freq matrix = (99, 677)
    Num of features identified = 677
    Topic 0:
    earth;son;sitting;thought;anymore;just;room;wasn;did;body
    Topic 1:
    knew;dog;blue;away;quickly;ran;know;said;test;saw
    Topic 2:
    went;stared;clever;came;day;turned;man;home;child;natalie
    Topic 3:
    knew;man;mother;strong;days;continued;realized;away;face;ve
    Topic 4:
    die;hours;mother;day;didn;later;blind;son;got;told


### story length distributions


```python
def count_words(text):
    words = re.findall(r'\w+', text)
    return len(words)
```


```python
tsh = twosentence.copy()
```


```python
tsh['title_length'] = tsh['title'].apply(count_words)
tsh['post_body_length'] = tsh['content'].apply(count_words)
tsh['combined'] = tsh['title_length'] + tsh['post_body_length']
```


```python
# Plot Title Length Distribution
plt.hist(tsh['title_length'], bins=20, color='skyblue', alpha=0.7)
plt.xlabel('Number of Words in Title')
plt.ylabel('Frequency')
plt.title('Title Length Distribution')
plt.show()

# Plot Post Body Length Distribution
plt.hist(tsh['post_body_length'], bins=20, color='lightcoral', alpha=0.7)
plt.xlabel('Number of Words in Post Body')
plt.ylabel('Frequency')
plt.title('Post Body Length Distribution')
plt.show()

# Plot Combined Length Distribution
plt.hist(tsh['combined'], bins=20, color='olive', alpha=0.7)
plt.xlabel('Total Number of Words (Title + Post Body)')
plt.ylabel('Frequency')
plt.title('Combined Length Distribution')
plt.show()

```


![png](output_98_0.png)



![png](output_98_1.png)



![png](output_98_2.png)


## Task 9: Write a summary of your findings!

## Write your summary in this cell
## --------------------------------


--Distribution Plots--
Looking at the distribution of compound sentiments, we see immediately how balanced (or skewed) our datasets are according to their shape. I was a little disappointed to see so many positively-scored posts in the r/solotravel dataset. I was hoping to see a slight more mix of positives and negatives. This goes the same for the r/dataengineering subreddit


--{solotravel} sentiment analysis by location--
it was interesting to see which geographic areas mentioned ranked the most/least by users in the solotravel subreddit based on their compound score. i made sure to set the minumum post count threshold to 5 to ensure that there is sufficient sample for a somewhat general consensus. it is not fair to have 1 negative review of a country/place represent the whole country in this analysis if there was only 1 post about said country! it was also interesting to see the bias that the subreddit seemingly had towards european travel destinations as opposed to non-european travel destinations, namely asia. 
i examined this disparity close and expanded it to representing the range of values via boxplots - building off the absolute average scores. what we see then is geographic locations who have strong vs weak consensus (pos, neg, neu, and compound) based on the box length. for example, from the boxplots, we can confirm that the european cities of Berlin, Venice, and Madrid are squarely considered among users in the subreddit to be associated with a positive experience.



---{data engingeer} sentiments by topic buckets---
similar to what was done for solotravel dataset, i grouped up the contents of each individual post into selected topics. from there, we can see how posts relating to or talking about said selected topics are generally ranked based on sentimental polarity scores. for the most part, posts relating to "projects", "career", and "company" are fairly positive. an interesting point about the "people" topic bucket is that while it is positive on average, it has a longer body than the rest, which means there are also mixed sentiments. is this representative of the actual data? i don't think so because "people" is a very general term and it can mean many different things based on context. so without context, i would argue that it is not conveying much as opposed to the other topic buckets.

then, i decided to do the same for data tools topics. it was really cool to see that Snowflake is the most well-received out of the bunch when people mentioned it in their posts - judging by the median from the compound boxplot. conversely, apache appears to be less than well-received but still relatively positive. databricks on the other hand is fairly positive but runs into the familiar problem of having a lot of mixed reviews to it.



---{covid19} top bri-grams over time---
for the covid19 subreddit, i knew i wanted to see time being an important element in my analysis. so i conducted a time-series plot, using week's rather than months, to analyze the shift in bi-gram frequencies or mentions over time. from the scatter plot, we can see that covid-related bigrams are showing up in recent weeks - the right side of the plot around where we are in the year (october 2023). but i wouldn't be alarmed as these are mere bi-grams and not covid tests themselves which means this is just an indicator of how many times covid-related discussion is being brought up. what's also interesting about this scatterplot is the seeming gap in discussion around week 20 mark - which around march and april of last year. so around spring time of last year, there was a gap of covid-related discussion among users and submissions alike in the covid19 subreddit - interesting!


--{two sentence horror}-- 
for this rather niche subreddit, i wanted to see the story length distribution of the dataset, and where the majority of posts landed. it was surprising to see a normal distribution based on the histogram plots i created - at least for the total count.  i realize the histograms are also correctly pointing out outliers in the data, so removing those would certainly grant us a definitive normal distribution around the center (40 word length).



all in all, i think this was a fantastic project to perform light NLP and text analysis given the time constraint. i feel like i have only just scratched the surface with my insights. thank you for reading.

```python

```

# Save your notebook, then `File > Close and Halt`

---
